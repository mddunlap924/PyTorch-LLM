{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "- Author: Myles Dunlap\n",
    "\n",
    "\n",
    "This notebook is used to train a model using a single configuration file. The steps in this notebook are used in the Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path for the custom modules\n",
    "path_custom_modules = '../'\n",
    "\n",
    "# Path to the YAML config. file\n",
    "path_cfg = {'base_dir': '../cfgs',\n",
    "            'filename': 'train-0.yaml'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import argparse\n",
    "import inspect\n",
    "\n",
    "# Append Path to Custom Modules\n",
    "sys.path.append(path_custom_modules)\n",
    "\n",
    "# Custom Modules\n",
    "from src.models import llm_multiclass\n",
    "from src.utils import (RecursiveNamespace,\n",
    "                       seed_everything,\n",
    "                       load_cfg,\n",
    "                       RunIDs)\n",
    "from src.dataloading.load_data import LoadData\n",
    "from src.dataloading.stratify import StratifyData\n",
    "from src.dataloading.preprocess import PreprocessData\n",
    "from src.dataloading.load_datasets import (TrainDataset,\n",
    "                                           CustomTextCollator,\n",
    "                                           )\n",
    "\n",
    "# Allow HF tokenizer parallelism\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'True'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration file\n",
    "CFG = load_cfg(base_dir=Path(path_cfg['base_dir']),\n",
    "               filename=path_cfg['filename'])\n",
    "\n",
    "# Set random seed on everything\n",
    "seed_everything(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group ID: f96d4ac2\n",
      "ID for Testing Fold #1: c35b8d3c\n",
      "\tFull/Entire ID: f96d4ac2-c35b8d3c\n",
      "ID for Testing Fold #2: 2757f8cb\n",
      "\tFull/Entire ID: f96d4ac2-2757f8cb\n",
      "ID for Testing Fold #3: 8fceedf3\n",
      "\tFull/Entire ID: f96d4ac2-8fceedf3\n",
      "ID for Testing Fold #4: eef8284a\n",
      "\tFull/Entire ID: f96d4ac2-eef8284a\n",
      "ID for Testing Fold #5: 2a0391f9\n",
      "\tFull/Entire ID: f96d4ac2-2a0391f9\n"
     ]
    }
   ],
   "source": [
    "# Group ID and ID for each fold tested\n",
    "run_ids = RunIDs(test_folds=CFG.cv.val_folds,\n",
    "                 num_folds=CFG.cv.num_folds)\n",
    "run_ids.generate_run_ids()\n",
    "\n",
    "# Print the group id and ids for each fold\n",
    "print(f'Group ID: {run_ids.group_id}')\n",
    "for fold_num in CFG.cv.val_folds:\n",
    "    fold_id = getattr(run_ids.folds_id,\n",
    "                      f'fold{fold_num}').run_id\n",
    "    entire_id = f'{run_ids.group_id}-{fold_id}' \n",
    "    print((f'ID for Testing Fold #{fold_num}: '\n",
    "           f'{fold_id}\\n\\tFull/Entire ID: {entire_id}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from Disk\n",
    "load_data_file = LoadData(base_dir=CFG.paths.data.base_dir)\n",
    "if CFG.debug:\n",
    "    data = load_data_file.load(filename=CFG.paths.data.debug_data)\n",
    "else:\n",
    "    data = load_data_file.load(filename=CFG.paths.data.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>State</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Product</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Summer of XX/XX/2018 I was denied a mortga...</td>\n",
       "      <td>IL</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There are many mistakes appear in my report wi...</td>\n",
       "      <td>VA</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are many mistakes appear in my report wi...</td>\n",
       "      <td>TX</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are many mistakes appear in my report wi...</td>\n",
       "      <td>TX</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There are many mistakes appear in my report wi...</td>\n",
       "      <td>CA</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Consumer complaint narrative State  \\\n",
       "0  The Summer of XX/XX/2018 I was denied a mortga...    IL   \n",
       "1  There are many mistakes appear in my report wi...    VA   \n",
       "2  There are many mistakes appear in my report wi...    TX   \n",
       "3  There are many mistakes appear in my report wi...    TX   \n",
       "4  There are many mistakes appear in my report wi...    CA   \n",
       "\n",
       "  Company response to consumer  \\\n",
       "0      Closed with explanation   \n",
       "1      Closed with explanation   \n",
       "2      Closed with explanation   \n",
       "3      Closed with explanation   \n",
       "4      Closed with explanation   \n",
       "\n",
       "                                             Product  fold  \n",
       "0  Credit reporting, credit repair services, or o...     1  \n",
       "1  Credit reporting, credit repair services, or o...     1  \n",
       "2  Credit reporting, credit repair services, or o...     1  \n",
       "3  Credit reporting, credit repair services, or o...     1  \n",
       "4  Credit reporting, credit repair services, or o...     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of a Product for Each Fold\n",
      "Notice how the quantities are evenly distributed across folds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Product                  fold\n",
       "Bank account or service  1       2953\n",
       "                         2       2953\n",
       "                         3       2954\n",
       "                         4       2953\n",
       "                         5       2953\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stratify the Data\n",
    "data = (StratifyData(technique=CFG.stratify.technique,\n",
    "                     n_folds=CFG.cv.num_folds,\n",
    "                     target=CFG.data_info.target)\n",
    "            .stratify(df=data))\n",
    "cols = CFG.data_info.source_fields + \\\n",
    "       [CFG.data_info.target, 'fold']\n",
    "display(data[cols].head(5))\n",
    "print(f'Distribution of a Product for Each Fold')\n",
    "print(f'Notice how the quantities are evenly distributed across folds')\n",
    "display(data.groupby('Product').fold.value_counts()\n",
    "        .sort_index().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start looping over folds here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Number of Instances: 305,739\n",
      "Validation Number of Instances: 76,435\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Train a model for each validation fold\n",
    "fold_num = CFG.cv.val_folds[0]\n",
    "\n",
    "# Split Data into Training and Validation\n",
    "df_train = data.copy()[data.fold != fold_num].reset_index(drop=True)\n",
    "df_val = data.copy()[data.fold == fold_num].reset_index(drop=True)\n",
    "print(f'Train Number of Instances: {len(df_train):,}')\n",
    "print(f'Validation Number of Instances: {len(df_val):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Target\n",
    "\n",
    "Convert the text target into a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Encoders\n",
    "encoders = {}\n",
    "for technique in CFG.preprocessing.apply_techniques:\n",
    "    fields = getattr(CFG.preprocessing, technique).fields\n",
    "    for col in fields:\n",
    "        enc = PreprocessData(y=df_train[col].values,\n",
    "                             technique=technique)\n",
    "        encoders[col] = {'encoder': enc.encoder,\n",
    "                         'technique': technique}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and Collator\n",
    "\n",
    "A collator is an object that forms a batch of data by using a list of dataset elements as inputs. A custom text collator is used here and it has several benefits such as:\n",
    "- the data does **NOT** have to be tokenizer prior to being passed to the PyTorch DataLoader class. This means the text data is tokenized once a batch of data is processed in in the DataLoader class and it allows for RAM usage to stay lower. For example, if your dataset exceeds the available RAM size then tokenizing all the data prior to the DataLoader (e.g., in the Dataset class) it will become problematic.\n",
    "- This enables dynamic padding over the batches. Later the maximum token length for a few batches will be printed to illustrate the dynamic padding. This ultimately leads to faster processing of data as explain in this [HuggingFace YouTube video](https://www.youtube.com/watch?v=7q5NyFT8REg).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_tokenizer.base_dir}/'\n",
    "                                          f'{CFG.model_tokenizer.name}')\n",
    "\n",
    "# Collator\n",
    "collator = CustomTextCollator(tokenizer=tokenizer,\n",
    "                              tokenizer_cfg=CFG.tokenizer\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all mix-data type fields\n",
    "\n",
    "In this [blob post by Chris McCormick](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/) an interesting approach was taken to combine mixed data types which was to convert all categorical and numerical into the text feed into the LLM. This repository will try this technique ingle string that will be processed by the LLM. \n",
    "\n",
    "Another approach is only pass the unstructured text into the LLM, take its last layer output and combine with the other mixed data types into a dense layer. This type of approach seems more common and an example can be found on [Google Colab here](https://colab.research.google.com/drive/1F7COnwHqcLDPg_SS-oFgW3c2GPDWnS5Y#scrollTo=BAQFbN-wBpoz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Training Samples: 305,739\n",
      "# of Validation Samples: 76,435\n",
      "Batch Size: 4\n",
      "305,739 \\ 4 = 76,435\n",
      "Train DataLoader # of Iters: 76,435\n",
      "Val. DataLoader # of Iters: 19,109\n",
      "\n",
      "Dynamic Padding\n",
      "\tThe shape is [batch size, maximum token length]\n",
      "Batch 0 of 76,435: torch.Size([4, 353])\n",
      "Batch 1 of 76,435: torch.Size([4, 325])\n",
      "Batch 2 of 76,435: torch.Size([4, 256])\n",
      "Batch 3 of 76,435: torch.Size([4, 220])\n",
      "Batch 4 of 76,435: torch.Size([4, 512])\n",
      "Batch 5 of 76,435: torch.Size([4, 512])\n",
      "Batch 6 of 76,435: torch.Size([4, 209])\n",
      "Batch 7 of 76,435: torch.Size([4, 512])\n",
      "Batch 8 of 76,435: torch.Size([4, 264])\n",
      "Batch 9 of 76,435: torch.Size([4, 470])\n",
      "Batch 10 of 76,435: torch.Size([4, 267])\n"
     ]
    }
   ],
   "source": [
    "# Train Dataset and Dataloader\n",
    "train_dataset = TrainDataset(df=df_train,\n",
    "                             tok=tokenizer,\n",
    "                             tok_cfg=CFG.tokenizer,\n",
    "                             X_cols=CFG.data_info.source_fields,\n",
    "                             label=CFG.data_info.target,\n",
    "                             encoder=encoders[CFG.data_info.target]['encoder'])\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              collate_fn=collator,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers,\n",
    "                              pin_memory=True)\n",
    "\n",
    "# Validation Dataset and Dataloader\n",
    "val_dataset = TrainDataset(df=df_val,\n",
    "                           tok=tokenizer,\n",
    "                           tok_cfg=CFG.tokenizer,\n",
    "                           X_cols=CFG.data_info.source_fields,\n",
    "                           label=CFG.data_info.target,\n",
    "                           encoder=encoders[CFG.data_info.target]['encoder'])\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=CFG.batch_size,\n",
    "                            collate_fn=collator,\n",
    "                            shuffle=True,\n",
    "                            num_workers=CFG.num_workers,\n",
    "                            pin_memory=True)\n",
    "\n",
    "print(f'# of Training Samples: {len(df_train):,}')\n",
    "print(f'# of Validation Samples: {len(df_val):,}')\n",
    "print(f'Batch Size: {CFG.batch_size}')\n",
    "print(f'{len(df_train):,} \\ {CFG.batch_size:,} = {len(train_dataloader):,}')\n",
    "print(f'Train DataLoader # of Iters: {len(train_dataloader):,}')\n",
    "print(f'Val. DataLoader # of Iters: {len(val_dataloader):,}')\n",
    "\n",
    "# Dynamic Padding of Maximum Token Lengths\n",
    "print(f'\\nDynamic Padding\\n\\tThe shape is [batch size, maximum token length]')\n",
    "for i, inputs in enumerate(train_dataloader):\n",
    "    if i > 10:\n",
    "        break\n",
    "    else:\n",
    "        print((f'Batch {i + 1} of {len(train_dataloader):,}: '\n",
    "               f'{inputs[\"input_ids\"].shape}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "- Generic HF info. on Bert: [HuggingFace Bert](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "- HF Model Card for: [bert-base-uncased](https://huggingface.co/bert-base-uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../hf_download/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoConfig,\n",
    "                          AutoModel,\n",
    "                          BertModel,\n",
    "                          BertForPreTraining,\n",
    "                          AutoModelForMaskedLM,\n",
    "                          BertForMaskedLM)\n",
    "model_cfg = (AutoConfig\n",
    "             .from_pretrained(Path(CFG.model_tokenizer.base_dir) / CFG.model_tokenizer.name))\n",
    "model = (BertModel\n",
    "         .from_pretrained(Path(CFG.model_tokenizer.base_dir) / CFG.model_tokenizer.name,\n",
    "                          config=model_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\t1: False\n",
      "\t2: False\n",
      "\t3: False\n",
      "\t4: False\n",
      "\t5: False\n",
      "2: ModuleList(\n",
      "  (0-9): 10 x BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\t1: False\n",
      "\t2: False\n",
      "\t3: False\n",
      "\t4: False\n",
      "\t5: False\n",
      "\t6: False\n",
      "\t7: False\n",
      "\t8: False\n",
      "\t9: False\n",
      "\t10: False\n",
      "\t11: False\n",
      "\t12: False\n",
      "\t13: False\n",
      "\t14: False\n",
      "\t15: False\n",
      "\t16: False\n",
      "\t17: False\n",
      "\t18: False\n",
      "\t19: False\n",
      "\t20: False\n",
      "\t21: False\n",
      "\t22: False\n",
      "\t23: False\n",
      "\t24: False\n",
      "\t25: False\n",
      "\t26: False\n",
      "\t27: False\n",
      "\t28: False\n",
      "\t29: False\n",
      "\t30: False\n",
      "\t31: False\n",
      "\t32: False\n",
      "\t33: False\n",
      "\t34: False\n",
      "\t35: False\n",
      "\t36: False\n",
      "\t37: False\n",
      "\t38: False\n",
      "\t39: False\n",
      "\t40: False\n",
      "\t41: False\n",
      "\t42: False\n",
      "\t43: False\n",
      "\t44: False\n",
      "\t45: False\n",
      "\t46: False\n",
      "\t47: False\n",
      "\t48: False\n",
      "\t49: False\n",
      "\t50: False\n",
      "\t51: False\n",
      "\t52: False\n",
      "\t53: False\n",
      "\t54: False\n",
      "\t55: False\n",
      "\t56: False\n",
      "\t57: False\n",
      "\t58: False\n",
      "\t59: False\n",
      "\t60: False\n",
      "\t61: False\n",
      "\t62: False\n",
      "\t63: False\n",
      "\t64: False\n",
      "\t65: False\n",
      "\t66: False\n",
      "\t67: False\n",
      "\t68: False\n",
      "\t69: False\n",
      "\t70: False\n",
      "\t71: False\n",
      "\t72: False\n",
      "\t73: False\n",
      "\t74: False\n",
      "\t75: False\n",
      "\t76: False\n",
      "\t77: False\n",
      "\t78: False\n",
      "\t79: False\n",
      "\t80: False\n",
      "\t81: False\n",
      "\t82: False\n",
      "\t83: False\n",
      "\t84: False\n",
      "\t85: False\n",
      "\t86: False\n",
      "\t87: False\n",
      "\t88: False\n",
      "\t89: False\n",
      "\t90: False\n",
      "\t91: False\n",
      "\t92: False\n",
      "\t93: False\n",
      "\t94: False\n",
      "\t95: False\n",
      "\t96: False\n",
      "\t97: False\n",
      "\t98: False\n",
      "\t99: False\n",
      "\t100: False\n",
      "\t101: False\n",
      "\t102: False\n",
      "\t103: False\n",
      "\t104: False\n",
      "\t105: False\n",
      "\t106: False\n",
      "\t107: False\n",
      "\t108: False\n",
      "\t109: False\n",
      "\t110: False\n",
      "\t111: False\n",
      "\t112: False\n",
      "\t113: False\n",
      "\t114: False\n",
      "\t115: False\n",
      "\t116: False\n",
      "\t117: False\n",
      "\t118: False\n",
      "\t119: False\n",
      "\t120: False\n",
      "\t121: False\n",
      "\t122: False\n",
      "\t123: False\n",
      "\t124: False\n",
      "\t125: False\n",
      "\t126: False\n",
      "\t127: False\n",
      "\t128: False\n",
      "\t129: False\n",
      "\t130: False\n",
      "\t131: False\n",
      "\t132: False\n",
      "\t133: False\n",
      "\t134: False\n",
      "\t135: False\n",
      "\t136: False\n",
      "\t137: False\n",
      "\t138: False\n",
      "\t139: False\n",
      "\t140: False\n",
      "\t141: False\n",
      "\t142: False\n",
      "\t143: False\n",
      "\t144: False\n",
      "\t145: False\n",
      "\t146: False\n",
      "\t147: False\n",
      "\t148: False\n",
      "\t149: False\n",
      "\t150: False\n",
      "\t151: False\n",
      "\t152: False\n",
      "\t153: False\n",
      "\t154: False\n",
      "\t155: False\n",
      "\t156: False\n",
      "\t157: False\n",
      "\t158: False\n",
      "\t159: False\n",
      "\t160: False\n"
     ]
    }
   ],
   "source": [
    "# # a = model.base_model\n",
    "# # # print(a.embeddings)\n",
    "# # len(a.encoder.layer)\n",
    "# for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#     print(f'{i + 1}: {param.requires_grad}')\n",
    "if model.freeze.apply:\n",
    "    freeze_layers = 4\n",
    "    model = model.base_model\n",
    "    modules = [model.embeddings, model.encoder.layer[:4]] #Replace 5 by what you want\n",
    "    for i, module in enumerate(modules):\n",
    "        print(f'{i + 1}: {module}')\n",
    "        for ii, param in enumerate(module.parameters()):\n",
    "            param.requires_grad = False\n",
    "            print(f'\\t{ii + 1}: {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('embeddings.word_embeddings.weight',\n",
       " Parameter containing:\n",
       " tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
       "         [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
       "         [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
       "         ...,\n",
       "         [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
       "         [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
       "         [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(18):\n",
    "#     a, b = next(iter(train_dataset), i)\n",
    "#     print(a['input_ids'].shape)\n",
    "#     print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dataset))\n",
    "\n",
    "# # Train a model for each validation fold\n",
    "# for fold_num in CFG.stratify.val_folds:\n",
    "#     print(f'Starting Training for Fold {fold_num}')\n",
    "    \n",
    "#     # Training Module\n",
    "    \n",
    "#     # Inference Module\n",
    "    \n",
    "#     print(f'\\tFinished Training for Fold {fold_num}')\n",
    "# print('Training and Validation Completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
