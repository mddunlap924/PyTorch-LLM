{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "- Author: Myles Dunlap\n",
    "\n",
    "\n",
    "This notebook is used to train a model using a single configuration file. The steps in this notebook are used in the Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path for the custom modules\n",
    "path_custom_modules = '../'\n",
    "\n",
    "# Path to the YAML config. file\n",
    "path_cfg = {'base_dir': '../cfgs',\n",
    "            'filename': 'train-0.yaml'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import argparse\n",
    "import inspect\n",
    "\n",
    "# Append Path to Custom Modules\n",
    "sys.path.append(path_custom_modules)\n",
    "\n",
    "# Custom Modules\n",
    "from src.models import llm_multiclass\n",
    "from src.utils import (RecursiveNamespace,\n",
    "                       seed_everything,\n",
    "                       load_cfg,\n",
    "                       RunIDs)\n",
    "from src.dataloading.load_data import LoadData\n",
    "from src.dataloading.stratify import StratifyData\n",
    "from src.dataloading.preprocess import PreprocessData\n",
    "\n",
    "# Allow HF tokenizer parallelism\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration file\n",
    "CFG = load_cfg(base_dir=Path(path_cfg['base_dir']),\n",
    "               filename=path_cfg['filename'])\n",
    "\n",
    "# Set random seed on everything\n",
    "seed_everything(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group ID: e0fde485\n",
      "ID for Testing Fold #1: 2acbab2e\n",
      "\tFull/Entire ID: e0fde485-2acbab2e\n",
      "ID for Testing Fold #2: 96333769\n",
      "\tFull/Entire ID: e0fde485-96333769\n",
      "ID for Testing Fold #3: b0e81d31\n",
      "\tFull/Entire ID: e0fde485-b0e81d31\n",
      "ID for Testing Fold #4: 2f30fa69\n",
      "\tFull/Entire ID: e0fde485-2f30fa69\n",
      "ID for Testing Fold #5: 0f4a10d7\n",
      "\tFull/Entire ID: e0fde485-0f4a10d7\n"
     ]
    }
   ],
   "source": [
    "# Group ID and ID for each fold tested\n",
    "run_ids = RunIDs(test_folds=CFG.cv.val_folds,\n",
    "                 num_folds=CFG.cv.num_folds)\n",
    "run_ids.generate_run_ids()\n",
    "\n",
    "# Print the group id and ids for each fold\n",
    "print(f'Group ID: {run_ids.group_id}')\n",
    "for fold_num in CFG.cv.val_folds:\n",
    "    fold_id = getattr(run_ids.folds_id,\n",
    "                      f'fold{fold_num}').run_id\n",
    "    entire_id = f'{run_ids.group_id}-{fold_id}' \n",
    "    print((f'ID for Testing Fold #{fold_num}: '\n",
    "           f'{fold_id}\\n\\tFull/Entire ID: {entire_id}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from Disk\n",
    "load_data_file = LoadData(base_dir=CFG.paths.data.base_dir)\n",
    "if CFG.debug:\n",
    "    data = load_data_file.load(filename=CFG.paths.data.debug_data)\n",
    "else:\n",
    "    data = load_data_file.load(filename=CFG.paths.data.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>State</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Product</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Summer of XX/XX/2018 I was denied a mortga...</td>\n",
       "      <td>IL</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There are many mistakes appear in my report wi...</td>\n",
       "      <td>VA</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are many mistakes appear in my report wi...</td>\n",
       "      <td>TX</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are many mistakes appear in my report wi...</td>\n",
       "      <td>TX</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There are many mistakes appear in my report wi...</td>\n",
       "      <td>CA</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Consumer complaint narrative State  \\\n",
       "0  The Summer of XX/XX/2018 I was denied a mortga...    IL   \n",
       "1  There are many mistakes appear in my report wi...    VA   \n",
       "2  There are many mistakes appear in my report wi...    TX   \n",
       "3  There are many mistakes appear in my report wi...    TX   \n",
       "4  There are many mistakes appear in my report wi...    CA   \n",
       "\n",
       "  Company response to consumer  \\\n",
       "0      Closed with explanation   \n",
       "1      Closed with explanation   \n",
       "2      Closed with explanation   \n",
       "3      Closed with explanation   \n",
       "4      Closed with explanation   \n",
       "\n",
       "                                             Product  fold  \n",
       "0  Credit reporting, credit repair services, or o...     1  \n",
       "1  Credit reporting, credit repair services, or o...     1  \n",
       "2  Credit reporting, credit repair services, or o...     1  \n",
       "3  Credit reporting, credit repair services, or o...     1  \n",
       "4  Credit reporting, credit repair services, or o...     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of a Product for Each Fold\n",
      "Notice how the quantities are evenly distributed across folds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Product                  fold\n",
       "Bank account or service  1       2953\n",
       "                         2       2953\n",
       "                         3       2954\n",
       "                         4       2953\n",
       "                         5       2953\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stratify the Data\n",
    "data = (StratifyData(technique=CFG.stratify.technique,\n",
    "                     n_folds=CFG.cv.num_folds,\n",
    "                     target=CFG.data_info.target)\n",
    "            .stratify(df=data))\n",
    "cols = CFG.data_info.source_fields + \\\n",
    "       [CFG.data_info.target, 'fold']\n",
    "display(data[cols].head(5))\n",
    "print(f'Distribution of a Product for Each Fold')\n",
    "print(f'Notice how the quantities are evenly distributed across folds')\n",
    "display(data.groupby('Product').fold.value_counts()\n",
    "        .sort_index().head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_tokenizer.base_dir}/'\n",
    "                                          f'{CFG.model_tokenizer.name}')\n",
    "\n",
    "# Collator\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer,\n",
    "                                   return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start looping over folds here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Number of Instances: 305,739\n",
      "Validation Number of Instances: 76,435\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Train a model for each validation fold\n",
    "fold_num = CFG.cv.val_folds[0]\n",
    "\n",
    "# Split Data into Training and Validation\n",
    "df_train = data.copy()[data.fold != fold_num].reset_index(drop=True)\n",
    "df_val = data.copy()[data.fold == fold_num].reset_index(drop=True)\n",
    "print(f'Train Number of Instances: {len(df_train):,}')\n",
    "print(f'Validation Number of Instances: {len(df_val):,}')\n",
    "# Preprocessing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Notes:\n",
    "- Preprocessing techniques should be fitted ONLY on TRAIN data. Then those fits will be applied to the VALIDATION data. Do not fit on both TRAIN and VALIDATION (this an cause data leakage).\n",
    "- The field `Company response to consumer` is a string and this will be combined with the unstructured text field `Consumer complaint narrative`.\n",
    "    - This step will be performed in the dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['State']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=CFG.preprocessing.apply_techniques[0]\n",
    "getattr(CFG.preprocessing, a).fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['Checking or savings account' 'Checking or savings account'\n 'Checking or savings account' ... 'Payday loan' 'Credit reporting'\n 'Mortgage'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m fields \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(CFG\u001b[39m.\u001b[39mpreprocessing, technique)\u001b[39m.\u001b[39mfields\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m fields:\n\u001b[0;32m----> 6\u001b[0m     enc \u001b[39m=\u001b[39m PreprocessData(y\u001b[39m=\u001b[39;49mdf_train[col]\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m      7\u001b[0m                         technique\u001b[39m=\u001b[39;49mtechnique)\n\u001b[1;32m      8\u001b[0m     encoders[col] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m'\u001b[39m: enc\u001b[39m.\u001b[39mencoder,\n\u001b[1;32m      9\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mtechnique\u001b[39m\u001b[39m'\u001b[39m: technique}\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(col)\n",
      "File \u001b[0;32m~/Projects/PyTorch-LLM/notebooks/../src/dataloading/preprocess.py:17\u001b[0m, in \u001b[0;36mPreprocessData.__init__\u001b[0;34m(self, y, technique)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m((\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEncoder needs to be added \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     15\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mto script: \u001b[39m\u001b[39m{\u001b[39;00mtechnique\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[1;32m     16\u001b[0m \u001b[39m# Fit the encoder\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m enc\u001b[39m.\u001b[39;49mfit(y)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Encoder\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m enc\n",
      "File \u001b[0;32m~/Projects/PyTorch-LLM/.venv/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/PyTorch-LLM/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:982\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    972\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    973\u001b[0m         (\n\u001b[1;32m    974\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`sparse` was renamed to `sparse_output` in version 1.2 and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse\n\u001b[0;32m--> 982\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    983\u001b[0m     X,\n\u001b[1;32m    984\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[1;32m    985\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    986\u001b[0m )\n\u001b[1;32m    987\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_drop_idx()\n\u001b[1;32m    988\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_features_outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_n_features_outs()\n",
      "File \u001b[0;32m~/Projects/PyTorch-LLM/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:77\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 77\u001b[0m X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(\n\u001b[1;32m     78\u001b[0m     X, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m n_features\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/PyTorch-LLM/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:43\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mPerform custom check_array:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m- convert list of strings to object dtype\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[39m# if not a dataframe, do normal check_array validation\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     X_temp \u001b[39m=\u001b[39m check_array(X, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite)\n\u001b[1;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(X_temp\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mstr_):\n\u001b[1;32m     45\u001b[0m         X \u001b[39m=\u001b[39m check_array(X, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m, force_all_finite\u001b[39m=\u001b[39mforce_all_finite)\n",
      "File \u001b[0;32m~/Projects/PyTorch-LLM/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    939\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    941\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    945\u001b[0m         )\n\u001b[1;32m    947\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(array\u001b[39m.\u001b[39mdtype, \u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    948\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    949\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    950\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    951\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['Checking or savings account' 'Checking or savings account'\n 'Checking or savings account' ... 'Payday loan' 'Credit reporting'\n 'Mortgage'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Preprocessing Encoders\n",
    "encoders = {}\n",
    "for technique in CFG.preprocessing.apply_techniques:\n",
    "    fields = getattr(CFG.preprocessing, technique).fields\n",
    "    for col in fields:\n",
    "        enc = PreprocessData(y=df_train[col].values,\n",
    "                            technique=technique)\n",
    "        encoders[col] = {'encoder': enc.encoder,\n",
    "                         'technique': technique}\n",
    "        print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encoders['Product']['encoder']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validation Dataloaders\n",
    "\n",
    "# # Train a model for each validation fold\n",
    "# for fold_num in CFG.stratify.val_folds:\n",
    "#     print(f'Starting Training for Fold {fold_num}')\n",
    "    \n",
    "#     # Training Module\n",
    "    \n",
    "#     # Inference Module\n",
    "    \n",
    "#     print(f'\\tFinished Training for Fold {fold_num}')\n",
    "# print('Training and Validation Completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
