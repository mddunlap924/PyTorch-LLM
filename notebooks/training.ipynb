{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "- Author: Myles Dunlap\n",
    "\n",
    "\n",
    "This notebook is used to train a model using a single configuration file. The steps in this notebook are used in the Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path for the custom modules\n",
    "path_custom_modules = '../'\n",
    "\n",
    "# Path to the YAML config. file\n",
    "path_cfg = {'base_dir': '../cfgs',\n",
    "            'filename': 'train-0.yaml'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import inspect\n",
    "\n",
    "# Append Path to Custom Modules\n",
    "sys.path.append(path_custom_modules)\n",
    "\n",
    "# Custom Modules\n",
    "from src.models import llm_multiclass\n",
    "from src.utils import (RecursiveNamespace,\n",
    "                       seed_everything,\n",
    "                       load_cfg,\n",
    "                       RunIDs)\n",
    "from src.dataloading.load_data import LoadData\n",
    "from src.dataloading.stratify import StratifyData\n",
    "from src.dataloading.preprocess import PreprocessData\n",
    "from src.dataloading.load_datasets import (TrainDataset,\n",
    "                                           CustomTextCollator,\n",
    "                                           )\n",
    "\n",
    "# Allow HF tokenizer parallelism\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'True'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration file\n",
    "CFG = load_cfg(base_dir=Path(path_cfg['base_dir']),\n",
    "               filename=path_cfg['filename'])\n",
    "\n",
    "# Set random seed on everything\n",
    "seed_everything(seed=CFG.seed)\n",
    "\n",
    "# Get Device type for processing (CPU/GPU)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group ID and ID for each fold tested\n",
    "run_ids = RunIDs(test_folds=CFG.cv.val_folds,\n",
    "                 num_folds=CFG.cv.num_folds)\n",
    "run_ids.generate_run_ids()\n",
    "\n",
    "# Print the group id and ids for each fold\n",
    "print(f'Group ID: {run_ids.group_id}')\n",
    "for fold_num in CFG.cv.val_folds:\n",
    "    fold_id = getattr(run_ids.folds_id,\n",
    "                      f'fold{fold_num}').run_id\n",
    "    entire_id = f'{run_ids.group_id}-{fold_id}' \n",
    "    print((f'ID for Testing Fold #{fold_num}: '\n",
    "           f'{fold_id}\\n\\tFull/Entire ID: {entire_id}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from Disk\n",
    "load_data_file = LoadData(base_dir=CFG.paths.data.base_dir)\n",
    "if CFG.debug:\n",
    "    data = load_data_file.load(filename=CFG.paths.data.debug_data)\n",
    "else:\n",
    "    data = load_data_file.load(filename=CFG.paths.data.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify the Data\n",
    "data = (StratifyData(technique=CFG.stratify.technique,\n",
    "                     n_folds=CFG.cv.num_folds,\n",
    "                     target=CFG.data_info.target)\n",
    "            .stratify(df=data))\n",
    "cols = CFG.data_info.source_fields + \\\n",
    "       [CFG.data_info.target, 'fold']\n",
    "\n",
    "# Number of classes for downstream use\n",
    "N_CLASSES = data[CFG.data_info.target].nunique()\n",
    "\n",
    "# Print information\n",
    "display(data[cols].head(5))\n",
    "print(f'Distribution of a Product for Each Fold')\n",
    "print(f'Notice how the quantities are evenly distributed across folds')\n",
    "display(data.groupby('Product').fold.value_counts()\n",
    "        .sort_index().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start looping over folds here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Train a model for each validation fold\n",
    "fold_num = CFG.cv.val_folds[0]\n",
    "\n",
    "# Split Data into Training and Validation\n",
    "df_train = data.copy()[data.fold != fold_num].reset_index(drop=True)\n",
    "df_val = data.copy()[data.fold == fold_num].reset_index(drop=True)\n",
    "print(f'Train Number of Instances: {len(df_train):,}')\n",
    "print(f'Validation Number of Instances: {len(df_val):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Target\n",
    "\n",
    "Convert the text target into a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Encoders\n",
    "encoders = {}\n",
    "for technique in CFG.preprocessing.apply_techniques:\n",
    "    fields = getattr(CFG.preprocessing, technique).fields\n",
    "    for col in fields:\n",
    "        enc = PreprocessData(y=df_train[col].values,\n",
    "                             technique=technique)\n",
    "        encoders[col] = {'encoder': enc.encoder,\n",
    "                         'technique': technique}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and Collator\n",
    "\n",
    "A collator is an object that forms a batch of data by using a list of dataset elements as inputs. A custom text collator is used here and it has several benefits such as:\n",
    "- the data does **NOT** have to be tokenizer prior to being passed to the PyTorch DataLoader class. This means the text data is tokenized once a batch of data is processed in in the DataLoader class and it allows for RAM usage to stay lower. For example, if your dataset exceeds the available RAM size then tokenizing all the data prior to the DataLoader (e.g., in the Dataset class) it will become problematic.\n",
    "- This enables dynamic padding over the batches. Later the maximum token length for a few batches will be printed to illustrate the dynamic padding. This ultimately leads to faster processing of data as explain in this [HuggingFace YouTube video](https://www.youtube.com/watch?v=7q5NyFT8REg).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "# Path to the model and tokenizer model card saved on disk\n",
    "model_path = Path(CFG.model_tokenizer.base_dir) / CFG.model_tokenizer.name\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "\n",
    "# Collator\n",
    "collator = CustomTextCollator(tokenizer=tokenizer,\n",
    "                              tokenizer_cfg=CFG.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all mix-data type fields\n",
    "\n",
    "In this [blob post by Chris McCormick](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/) an interesting approach was taken to combine mixed data types which was to convert all categorical and numerical into the text feed into the LLM. This repository will try this technique ingle string that will be processed by the LLM. \n",
    "\n",
    "Another approach is only pass the unstructured text into the LLM, take its last layer output and combine with the other mixed data types into a dense layer. This type of approach seems more common and an example can be found on [Google Colab here](https://colab.research.google.com/drive/1F7COnwHqcLDPg_SS-oFgW3c2GPDWnS5Y#scrollTo=BAQFbN-wBpoz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset and Dataloader\n",
    "train_dataset = TrainDataset(df=df_train,\n",
    "                             tok=tokenizer,\n",
    "                             tok_cfg=CFG.tokenizer,\n",
    "                             X_cols=CFG.data_info.source_fields,\n",
    "                             label=CFG.data_info.target,\n",
    "                             encoder=encoders[CFG.data_info.target]['encoder'])\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              collate_fn=collator,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers,\n",
    "                              pin_memory=True,\n",
    "                              )\n",
    "\n",
    "# Validation Dataset and Dataloader\n",
    "val_dataset = TrainDataset(df=df_val,\n",
    "                           tok=tokenizer,\n",
    "                           tok_cfg=CFG.tokenizer,\n",
    "                           X_cols=CFG.data_info.source_fields,\n",
    "                           label=CFG.data_info.target,\n",
    "                           encoder=encoders[CFG.data_info.target]['encoder'])\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=CFG.batch_size,\n",
    "                            collate_fn=collator,\n",
    "                            shuffle=True,\n",
    "                            num_workers=CFG.num_workers,\n",
    "                            pin_memory=True,\n",
    "                            )\n",
    "\n",
    "print(f'# of Training Samples: {len(df_train):,}')\n",
    "print(f'# of Validation Samples: {len(df_val):,}')\n",
    "print(f'Batch Size: {CFG.batch_size}')\n",
    "print(f'{len(df_train):,} \\ {CFG.batch_size:,} = {len(train_dataloader):,}')\n",
    "print(f'Train DataLoader # of Iters: {len(train_dataloader):,}')\n",
    "print(f'Val. DataLoader # of Iters: {len(val_dataloader):,}')\n",
    "\n",
    "# Dynamic Padding of Maximum Token Lengths\n",
    "print(f'\\nDynamic Padding\\n\\tThe shape is [batch size, maximum token length]')\n",
    "for i, inputs in enumerate(train_dataloader):\n",
    "    if i > 10:\n",
    "        break\n",
    "    else:\n",
    "        print((f'Batch {i + 1} of {len(train_dataloader):,}: '\n",
    "               f'{inputs[\"input_ids\"].shape}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "- Generic HF info. on Bert: [HuggingFace Bert](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "- HF Model Card for: [bert-base-uncased](https://huggingface.co/bert-base-uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.llm_multiclass import CustomModel\n",
    "# Load custom model\n",
    "model = CustomModel(llm_model_path=model_path,\n",
    "                    cfg=CFG.model,\n",
    "                    num_classes=N_CLASSES)\n",
    "\n",
    "# Set model on device\n",
    "model.to(DEVICE)\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.optimizers import get_optimizer\n",
    "# Optimizer\n",
    "optimizer = get_optimizer(cfg=CFG.optimizer,\n",
    "                          model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of steps/iterations\n",
    "total_steps = CFG.epochs * len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduler\n",
    "\n",
    "Learning rate schedulers can be used to help an algorithm converge to a more optimal solution. Please refer to this references for more information and a visual representation of various learning rate schedules.\n",
    " - [Medium Article](https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)\n",
    " - [Kaggle Notebook](https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "# Learning rate scheduler\n",
    "scheduler = OneCycleLR(optimizer=optimizer,\n",
    "                       total_steps=total_steps,\n",
    "                       max_lr=CFG.optimizer.lr.max)\n",
    "lr_value = []\n",
    "steps = []\n",
    "step_count = 0\n",
    "for epoch in range(CFG.epochs):\n",
    "    for batch_count in range(len(train_dataloader)):\n",
    "        optimizer.step()\n",
    "        lr_value.append(optimizer.param_groups[0]['lr'])\n",
    "        steps.append(step_count)\n",
    "        scheduler.step()\n",
    "        step_count +=1\n",
    "\n",
    "# Re-establish optimizer and scheduler since they were called\n",
    "del optimizer, scheduler\n",
    "_ = gc.collect()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = get_optimizer(cfg=CFG.optimizer,\n",
    "                          model=model)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = OneCycleLR(optimizer=optimizer,\n",
    "                       total_steps=total_steps,\n",
    "                       max_lr=CFG.optimizer.lr.max)\n",
    "\n",
    "# # Plot Learning Rate Schedule\n",
    "# plt.plot(steps, lr_value)\n",
    "# plt.xlabel('Steps')\n",
    "# plt.ylabel('Learning Rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### Loss Functions References\n",
    "- [Machine Learning Mastery](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "- [Neptune AI Article](https://neptune.ai/blog/pytorch-loss-functions)\n",
    "- [PyTorch Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "Since this is a multi-class classification we can use nn.CrossEntropyLoss as the loss function. More elborate modules can be built for selecting and creating custom loss functions.\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "[TorchMetrics](https://torchmetrics.readthedocs.io/en/latest/) will be used for implementing performance metrics.\n",
    "- [F1-Score](https://torchmetrics.readthedocs.io/en/stable/classification/f1_score.html)\n",
    "- [Precision]()\n",
    "- [Recall]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from src.training.metrics import AverageMeter\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchmetrics import F1Score, Precision, Recall\n",
    "\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Performance metrics\n",
    "f1 = F1Score(task='multiclass', num_classes=N_CLASSES)\n",
    "precision = Precision(task='multiclass', num_classes=N_CLASSES)\n",
    "recall = Recall(task='multiclass', num_classes=N_CLASSES)\n",
    "\n",
    "# Training loop over epochs\n",
    "start_training_time = time.time()\n",
    "step_count = 0\n",
    "best_score = 0.0\n",
    "for epoch in range(CFG.epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f'\\nStart Epoch {epoch + 1}')\n",
    "    train_meters = {\n",
    "        'loss': AverageMeter(),\n",
    "        'f1': AverageMeter(),\n",
    "        'precision': AverageMeter(),\n",
    "        'recall': AverageMeter(),\n",
    "    }\n",
    "    model.train()\n",
    "    \n",
    "    # TRAINING\n",
    "    # Iterate over each batch in an epoch\n",
    "    # for idx, batch in enumerate(train_dataloader):\n",
    "    X = {'input_ids': batch['input_ids'].to(DEVICE),\n",
    "         'attention_mask': batch['attention_mask'].to(DEVICE)}\n",
    "    y = batch['labels'].to(DEVICE)\n",
    "\n",
    "    # Model prediction\n",
    "    y_pred = model(X)\n",
    "    # Calculate loss\n",
    "    loss = loss_fn(input=y_pred, target=y)\n",
    "    \n",
    "    # Backward pass, optimizer & scheduler steps\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Performance metrics for the batch of data\n",
    "    f1_score = f1(y_pred, y)\n",
    "    precision_score = precision(y_pred, y)\n",
    "    recall_score = recall(y_pred, y)\n",
    "\n",
    "    # Store loss and performance metrics\n",
    "    train_meters['loss'].update(loss, n=X.size(0))\n",
    "    train_meters['cos'].update(trn_cos, n=X.size(0))  \n",
    "    \n",
    "    # Progress bar info.\n",
    "    tk0.set_postfix(train_loss=float(train_meters['loss'].avg.detach().cpu().numpy()),\n",
    "                    train_cos=train_meters['cos'].avg,\n",
    "                    lr=scheduler.get_last_lr()[0])       \n",
    "    \n",
    "    # Log Iterations results in WandB\n",
    "    if ((step_count + 1) % 500) == 0:\n",
    "        wandb.log({'step': step_count,\n",
    "                    'train_loss': float(train_meters['loss'].avg.detach().cpu().numpy()),\n",
    "                    'train_cos': train_meters['cos'].avg,\n",
    "                    'lr': scheduler.get_last_lr()[0],\n",
    "                    })\n",
    "    step_count += 1\n",
    "print('Epoch {:d} / trn/loss={:.4f}, trn/cos={:.4f}'.format(\n",
    "    epoch + 1,\n",
    "    train_meters['loss'].avg,\n",
    "    train_meters['cos'].avg))\n",
    "print(f'Epoch {epoch + 1} Training Time: '\n",
    "        f'{(((time.time() - epoch_start_time) / 60) / 60):.1f} hrs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
